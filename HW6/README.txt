Problem 1 (50pts)

For the problem of Machine Translation problem: Train a deeper Transformer than what we did during the lectures.  
How does it affect the training speed, model complexity, and translation performance both quantitatively and qualitatively? 
Report and plot your results.

 

Problem 2 (50pts)

For the problem of the Vision Transformer, we need, in lectures, to train a deeper Transformer with more multiheaded 
self-attention blocks. How does it affect the training speed, model complexity, and validation accuracy? 
Report and plot your results.
