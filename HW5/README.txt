Problem 1 (40pts)

For the problem of Machine Translation using sequence-to-sequence model


Can you adjust the hyperparameters to improve the translation results?

If the encoder and the decoder differ in the number of layers or the number of hidden
units, how can we initialize the hidden state of the decoder? Please run an experiment 
for it with 3 layers for encoder and 2 layers for decoder. Plot your training results 
and compare it against the baseline examples from the lectures.

Rerun the baseline experiment by replacing GRU with LSTM. Plot your results and compare
GRU against LSTM.
 
 
 

Problem 2 (60pts)

For the problem of Machine Translation with Bahdanau attention based sequence-to-sequence 
modeling


Explore the impacts of number of hidden layers starting tom 1 hidden layer up to 4 hidden 
layers. Plot the results (training loss and validation), also run few examples to do the 
qualitative comparison between these two. Can you draw the attention weight matrixes and 
compare them.

Replace GRU with LSTM in the experiment. Perform training again. Plot the results (training
loss and validation), also run few examples to do the qualitative comparison between these 
two. Can you draw the attention weight matrixes and compare them.
 
